{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeee32c8",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/worldbank/dec-python-course/blob/main/2-advanced-topics/text-analysis/intro-text-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23677227",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis\n",
    "\n",
    "Text analysis is the process of extracting meaningful information from text data, uncovering insights that would otherwise remain buried under text corpora.\n",
    "\n",
    "This session is an **introduction** to text analysis. We'll be covering the following topics:\n",
    "\n",
    "1. Regex and character patterns in text data\n",
    "1. Text data pre-processing\n",
    "1. Counting words\n",
    "1. Sentiment analysis\n",
    "1. Text classification\n",
    "\n",
    "The session assumes previous knowledge of Python and Pandas, and some knowledge of data visualization using seaborn.\n",
    "\n",
    "We'll use the following libraries in this notebook:\n",
    "\n",
    "- **pandas** for dataframe operations\n",
    "- **re** for regular expressions\n",
    "- **spacy** for text data processing\n",
    "- **seaborn**, **matplotlib**, and **wordcloud** for data visualization\n",
    "- **nltk** for sentiment analysis\n",
    "- **sklearn** for data classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11190e",
   "metadata": {},
   "source": [
    "## (some) Data exploration\n",
    "\n",
    "We'll start by getting familiarized with our dataset. We'll use a structured tabular dataset of working papers obtained from the WB Documents API.\n",
    "\n",
    "Run the following line to load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ca0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/worldbank/dec-python-course/main/2-advanced-topics/text-analysis/data/papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85790a75",
   "metadata": {},
   "source": [
    "The data is a corpus of working papers from the WB Policy Research Working Paper series. For each paper, we have:\n",
    "\n",
    "- A paper identifier\n",
    "- The Title\n",
    "- Two URLs\n",
    "- The topics of the paper, separated by commas\n",
    "- An abstract\n",
    "- A text\n",
    "\n",
    "Let's take a closer look at the columns `url`, `url_text`, and `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6966058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716b1b6",
   "metadata": {},
   "source": [
    "`url` contains the paper URL, `url_text` is the URL to actual text content, and `text` is the text of the paper.\n",
    "\n",
    "Now that we know what the data is about, we can start planning what to do with it. In general, all the tasks we'll do are about data preparation, and basic descriptive and classification tasks. This is a summary of what we'll do:\n",
    "\n",
    "1. Generate new features (columns) based on the text\n",
    "1. Count the words and most used words\n",
    "1. Obtain the emotional tone (sentiment) of sentences\n",
    "1. Build a topic classifier from our corpus\n",
    "\n",
    "For the first task, we'll expand the columns of the dataset using existing patterns in the text.\n",
    "\n",
    "## Patterns\n",
    "\n",
    "Let's take another look at the text. This time we'll use the function `print()`, so that space characters are properly rendered and the text is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8be813",
   "metadata": {},
   "source": [
    "Note that there are a number of information elements that seem to follow some patterns in the text:\n",
    "\n",
    "- The WP number is in the last sequence of non-space characters in the first line\n",
    "- The authors' names is a series of contiguous lines after the paper title\n",
    "- Abstract: lines after the word \"Abstract\" in the beginnning of the text. All of them seem to have a big space in the middle of the sentence\n",
    "- Keywords: separated by a semi-colon in a line that starts with \"Keywords\"\n",
    "- JEL Codes: an uppercase caracter followed by two numbers, separated by commas\n",
    "- Authors emails: non-space sequence of characters with \"at\" sign (\"@\") and ending in \".org\", \".com\"\n",
    "- Bibliography elements: last lines of the text\n",
    "\n",
    "We're going to take advantage of the patterns of JEL codes to extract them in a new column and add them to our original dataframe. We'll use regular expressions for this.\n",
    "\n",
    "**Important:** We're only checking one observation (the first) when inferring these patterns. If you want to cretae a column for the dataframe and not for a single observation, you'd have to make sure the same pattern exists in the rest of the texts of your corpus. We'll take it for granted in this session for the sake of time, but you should note that manually exploring different observations of your corpus is needed to infer possible patterns in your texts.\n",
    "\n",
    "### Regular expressions\n",
    "\n",
    "In programming, regular expressions are sequences of characters that match a pattern in text. A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The ID number of participant 1 is 30551. They were born on July 01, 1996. Participant 2 has ID 71098.'\n",
    "\n",
    "# Pattern for capturing IDs in this text: sequences of five number characters:\n",
    "pattern = '\\d{5}'\n",
    "\n",
    "# Capturing IDs\n",
    "ids = re.findall(pattern, text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b0e4b",
   "metadata": {},
   "source": [
    "Some notes about this code:\n",
    "- `\\d` is a wildcard that represents one number (0-9). This is also the same as `[0-9]`\n",
    "- `{5}` means that the previous character in the pattern is repeated five times\n",
    "- A variation of this pattern could be `\\d{4}`, which could be used to capture years. This would have returned a list with `1996` in the example above\n",
    "\n",
    "In regex, there is a wildcard for almost everything. Some examples:\n",
    "\n",
    "- Character wildcards:\n",
    "    + `\\d` --> digits (0-9)\n",
    "    + `\\W` --> any word character (uppercase and lowercase a-z, digits, and underscore (\"_\") )\n",
    "    + `\\n` --> newline characters\n",
    "    + `\\s` --> whitespace characters, including newline\n",
    "    + `.` --> any character except newline\n",
    "- Character repetition:\n",
    "    + `{a}` --> the previous character, repeated \"a\" times\n",
    "    + `{a,b}` --> the previous character, repeated between \"a\" and \"b\" times\n",
    "    + `*` --> the previous character, repeated zero or more times\n",
    "    + `+` --> the previous character, repeated one or more times\n",
    "    \n",
    "Regex can match any pattern we can possibly imagine. However, working with regex can be complex for starters. For the purpose of this session, we've introduced regex so you know it exists and can be used to create columns in datasets containing corpus of documents. Don't worry for now if you still didn't grasp well how the patterns work, but if you're interested in learning more about rege, we recommend the following resources:\n",
    "\n",
    "- A nice regex tutorial is [here](https://regexone.com/)\n",
    "- A great regex visualizer tool is [here](https://jex.im/regulex/#!flags=&re=www%5C.%5Ba-zA-Z0-9-%5D%2B%5C.(%3F%3Acom%7Cnet%7Corg))\n",
    "\n",
    "### Extracting information using patterns\n",
    "\n",
    "Remember we said that JEL codes in the text looked like a pattern of one uppercase letters followed by two digits? We'll use this to extract the JEL codes of each paper in a new column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[A-Z]\\d{2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af17b44",
   "metadata": {},
   "source": [
    "This pattern captures one uppercase alphabetic character (`[A-Z]`), followed by one digit repeated two times (`\\d{2}`).\n",
    "\n",
    "Now we'll define a helper function that looks for this pattern in a text and returns all captures in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf537283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_jel(text):\n",
    "    \n",
    "    pattern = '[A-Z]\\d{2}'\n",
    "    result = re.findall(pattern, text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae412c",
   "metadata": {},
   "source": [
    "Lastly, we'll map this function using Pandas' `apply()` method to create a new column in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c373cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jel'] = df['text'].apply(capture_jel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f045d",
   "metadata": {},
   "source": [
    "Now we have a new column in our dataset. Great!\n",
    "\n",
    "For the next part of the session, we'll start properly analyzing and getting insights from the text contents. The final result of the next part will be a count of the most used words in each text and we'll also count the total number of words in each text during the process.\n",
    "\n",
    "## Text data pre-processing\n",
    "\n",
    "Before we start, we need to think of the following:\n",
    "\n",
    "- Our texts are in a very raw state. Shouldn't we \"clean\" them a bit before counting words?\n",
    "- Using regex to capture words so we can count them sounds possible, but perhaps there is an easier way?\n",
    "- Texts in English usually repeat a lot words that are not very insightful about the content, such as prepositions or pronouns. Can we get rid of some of them before the word count?\n",
    "- Lastly, shouldn't we count in the same category words that are not exactly the same but have a very similar meaning? for example:\n",
    "    + different conjugations of the same verb\n",
    "    + singular and plural forms of the same noun\n",
    "    \n",
    "The answer to all of these questions is Yes. We'll do this in the data pre-processing. Data pre-processing in text analysis is extremely important. Omitting pre-processing will give you different results in text analysis tasks.\n",
    "\n",
    "Data pre-processing can consist of multiple tasks. We'll apply the following for our corpus:\n",
    "\n",
    "- Transform to lowercase\n",
    "- Tokenization: transform texts into lists of words\n",
    "- Remove stop words (words that are not very insightful, such as prepositions)\n",
    "- Lemmatization: transform different forms of words into a common word that conveys a similar meaning. This is useful to \"normalize\" conjugations of verbs or plural forms of words\n",
    "\n",
    "Fortunately, there is a very useful Python library we can use for this: [spaCy](https://spacy.io/). SpaCy makes available pre-existing NLP models that tokenize, lemmatize, and detect stop words and non-word characters (such as digits or punctuation), so we can easily transform a text into a list of \"meaningful\" lemmatized words that we can use for word counts.\n",
    "\n",
    "### Working with spaCy\n",
    "\n",
    "First we need to install spaCy. Uncomment the line below, run it, and then comment it again with `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a9ba4",
   "metadata": {},
   "source": [
    "Now we need to **download** spaCy's NLP model. Uncomment the line below, run it only once, and then comment it out again to make sure you won't run it again accidentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639db72e",
   "metadata": {},
   "source": [
    "Now we **load** the model so it's available in this Python notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8608234",
   "metadata": {},
   "source": [
    "Then, we'll build a function that:\n",
    "\n",
    "1. Reads a text\n",
    "1. Transforms it to lowercase\n",
    "1. Loads it into the model\n",
    "1. For each word, obtains the lemmatized versions of words that are not:\n",
    "    - Stop words\n",
    "    - Punctuation\n",
    "    - Numbers\n",
    "    - Spaces\n",
    "1. Finally, the function returns a list of the lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6694c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization_normalization(text):\n",
    "    \n",
    "    text = text.lower() # lowercase\n",
    "    doc = nlp(text)     # loading text into model\n",
    "\n",
    "    words_normalized = []\n",
    "    for word in doc:\n",
    "        if word.text != '\\n' \\\n",
    "        and not word.is_stop \\\n",
    "        and not word.is_punct \\\n",
    "        and not word.like_num \\\n",
    "        and len(word.text.strip()) > 2:\n",
    "            word_lemmatized = str(word.lemma_)\n",
    "            words_normalized.append(word_lemmatized)\n",
    "    \n",
    "    return words_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e9f90",
   "metadata": {},
   "source": [
    "To get a better idea of what the function does, let's take a look at the result for one paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'][10]\n",
    "doc_tokenized = word_tokenization_normalization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990522a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254a18c",
   "metadata": {},
   "source": [
    "The result is a list of normalized words for the text.\n",
    "\n",
    "You might have also noticed that this takes some time to run. To avoid having to wait, we'll apply the function to tokenize and normalize only the **abstracts**. We'll again use the Pandas method `apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee78234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract_tokenized'] = df['abstract'].apply(word_tokenization_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb35728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829719f",
   "metadata": {},
   "source": [
    "The downside of having applied the tokenization and normalization on the abstracts and not the texts is that for word counts we might not have abstracts long enough to make word repetition very insightful. In a real project, we should have used the full texts, leave the code running while we do other things or go for coffee, and come back and work with the results once the code finishes.\n",
    "\n",
    "## Counting words\n",
    "\n",
    "Now that the texts are normalized, we can count words! We'll do two things:\n",
    "\n",
    "1. Generate a column with the number of words\n",
    "1. Generate a column with a dictionary where each word is a key and the number of times are the key's values. This will look like `{'word1': n1, 'word2': n2, ...}`\n",
    "\n",
    "For the first task, we can directly create a new column with the result in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df410125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_words_abstract'] = df['abstract_tokenized'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30076b94",
   "metadata": {},
   "source": [
    "Just out of curiosity, let's pause for a minute to see the distribution in the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3333cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this line if you don't have seaborn:\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7da923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f89e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='n_words_abstract');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc464a",
   "metadata": {},
   "source": [
    "For the second task, we need to generate a helper function that generates the dictionary from each tokenized abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45512e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts(tokenized_text):\n",
    "    \n",
    "    count = {}\n",
    "    \n",
    "    for word in tokenized_text:\n",
    "        if word in count:\n",
    "            count[word] += 1\n",
    "        else:\n",
    "            count[word] = 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0038b4",
   "metadata": {},
   "source": [
    "We'll first apply the function to only one text to make sure the result looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokenized = df['abstract_tokenized'][42]\n",
    "count = word_counts(abstract_tokenized)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0e55e",
   "metadata": {},
   "source": [
    "This looks interesting, but it's not very meaningful unless we spend some time looking at the result. We'll transform this into a barplot for easier interpretation but only keeping the words with more than 2 counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_trimmed = {}\n",
    "for word, value in count.items():\n",
    "    if value > 2:\n",
    "        count_trimmed[word] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = sorted(count_trimmed, key=lambda x:count_trimmed[x]) # we add this to see the result sorted (ascending)\n",
    "sns.barplot(count_trimmed, orient='h', order=sorting[::-1]);   # [::-1] reverses the order of a list, for descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3fa48",
   "metadata": {},
   "source": [
    "Now we'll apply the function `word_counts()` to all the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b94201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract_word_count'] = df['abstract_tokenized'].apply(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f443f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f5c74",
   "metadata": {},
   "source": [
    "The word count we just generated in `abstract_word_count` would be useful if we wanted to analyze the counts of an individual paper. In a corpus like this, however, it might be more useful to obtain a word count of all the papers we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending all lists in abstract_tokenized\n",
    "all_words = df['abstract_tokenized'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cedb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903c9b6",
   "metadata": {},
   "source": [
    "Now we'll apply our function to count words and save the result into a dictionary on `all_words`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_complete = word_counts(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd20c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c683e2",
   "metadata": {},
   "source": [
    "With this, we can plot the count of words for our entire corpus of papers. We'll do it below for the `n` words most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b547113",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "values_sorted = sorted(count_complete.values())[::-1] # This returns the values in count_complete, sorted in descending order.\n",
    "nth_value = values_sorted[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69673a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nth_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcda933",
   "metadata": {},
   "source": [
    "This means that after sorting our word counts descending, 209 is the value in the 16th position --remember that positions in Python are always zero-indexed. We'll traverse through our dictionary and will keep only the counts higher than this value, saving the result in a new dictionary called `count_complete_trimmed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_complete_trimmed = {}\n",
    "for word, value in count_complete.items():\n",
    "    if value > nth_value:\n",
    "        count_complete_trimmed[word] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_complete_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_complete_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10272cf3",
   "metadata": {},
   "source": [
    "Now we can produce our plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = sorted(count_complete_trimmed, key=lambda x:count_complete_trimmed[x]) # we add this to see the result sorted\n",
    "sns.barplot(count_complete_trimmed, orient='h', order=sorting[::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764612e",
   "metadata": {},
   "source": [
    "### Word clouds\n",
    "\n",
    "What kind of text analysis training would this be without a word cloud example? we'll use our dictionary of word counts for the corpus of papers and the library `wordcloud` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate and run the line below only once to install wordcloud\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12beb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white', colormap = 'binary').generate_from_frequencies(count_complete)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc937d4d",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Sentiment analysis consists of determining the emotional tone of a text. It classifies a text into one of three types: positive, neutral, or negative sentiment.\n",
    "\n",
    "Sentiment analysis works best when it's applied on sentences. However, the unit of observation of our dataframe and corpus is a paper. Before moving forward, we'll transform our dataframe from a paper level to a sentence level.\n",
    "\n",
    "### Sentence-level tokenization\n",
    "\n",
    "To divide texts into sentences, we'll use the sentencizer from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenization(text):\n",
    "    \n",
    "    text = text.lower() # lowercase\n",
    "    doc = nlp(text)     # loading text into model\n",
    "    sentences = [sentence.text for sentence in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb28382",
   "metadata": {},
   "source": [
    "We'll now try this function with one abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c56571",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['abstract'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentence_tokenization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8818ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1dad4",
   "metadata": {},
   "source": [
    "The result has text that could be cleaned a bit more (for example: removing line breaks and replacing multiple contiguous spaces with only one space), but we're going to omit that for now. This sentence separation is good enough for sentiment analysis.\n",
    "\n",
    "Now we'll apply the separation to all abstracts. We use the abstracts for this example because executing the function on all paper texts would take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract_sentencized'] = df['abstract'].apply(sentence_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8080c",
   "metadata": {},
   "source": [
    "We said before that we wanted to work with a dataframe at the sentence level. We use Pandas' method `.explode()` to obtain this result easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence = df[['id', 'abstract_sentencized']]  # leaving only the paper ID and the setencized abstract\n",
    "df_sentence = df_sentence.explode('abstract_sentencized').reset_index(drop=True) # converting DF to sentence-level\n",
    "df_sentence = df_sentence.rename({'abstract_sentencized': 'sentence'}, axis='columns') # renaming column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273dcaf4",
   "metadata": {},
   "source": [
    "## Obtaining sentiments\n",
    "\n",
    "There are several libraries with pre-loaded models that analyze the sentiment of a text in English or other languages. We'll use `nltk` because it's one of the simplest. For texts in other languages, you can check [spaCy's language models](https://spacy.io/usage/models). Most of them have a model method for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe87c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate and run this line to install nltk\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14249d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fbd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This downloads nltk's model for sentiment analysis. Activate this line and run it only once\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22200b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f532f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'm having a terrible day\"\n",
    "analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is excellent!'\n",
    "analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ca128",
   "metadata": {},
   "source": [
    "Most sentiment analysis models don't give a final result on whether a text is positive, negative, or neutral; they give scores for each sentiment. We'll apply a simple rule for these scores to determine the tone of a text: the highest score is the tone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec732b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \n",
    "    result = analyzer.polarity_scores(text)\n",
    "    \n",
    "    if result['neg'] > result['pos'] and result['neg'] > result['neu']:\n",
    "        return 'negative'\n",
    "    elif result['pos'] > result['neg'] and result['pos'] > result['neu']:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence['sentiment'] = df_sentence['sentence'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f5db7",
   "metadata": {},
   "source": [
    "Visualizing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df_sentence['sentiment'].value_counts());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba06a9",
   "metadata": {},
   "source": [
    "The overwhelming majority of sentences in the abstracts have a neutral tone. This is exactly what we would expect of a corpus like this. We'll tabulate the results to see if there are any positive- or negative-sentiment sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b51c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65ecbc",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "\n",
    "For the last part of the session, we'll do a couple of simple text classification examples. We're calling them \"simple\" because there are now very fancy and state-of-the-art text classification techniques for text, but that are not suitable for a 2-hour session. You can check the link listed below about LLMs if you want to explore more about these.\n",
    "\n",
    "Simply put, text classification consists of assigning a text to a group. If you're familiar with machine learning, this is exactly a machine learning classification task. For our exercises, we'll show two ways of classifying text:\n",
    "- **Unsupervised classification:** we'll group texts into groups of similarity, without pre-defining the groups\n",
    "- **Supervised classification:** we'll group texts into pre-defined groups. The pre-defined groups will be the first topic of the column `topics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e26877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_topic'] = df['topics'].apply(lambda x: x.split(',')[0].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a5119",
   "metadata": {},
   "source": [
    "Now we'll tabulate the first topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12294ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['first_topic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b815435",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8432404a",
   "metadata": {},
   "source": [
    "There are 198 topics for a total of 399 papers (!), which means that a lot of topics have only one or two papers. We'll keep only topics that have at least eight papers so that there is at least some observations in each topic to build a classifier. This will reduce the size of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11aa2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_to_keep = df['first_topic'].value_counts()[df['first_topic'].value_counts() >= 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_to_keep.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e4af5",
   "metadata": {},
   "source": [
    "Our resulting dataframe will have only have 95 observations and 8 topics. This numbers of obs is not enough to generate a good classifier but we'll still go ahead and use it for the exercise as an example of the application of the text classification method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f330472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['first_topic'].isin(topics_to_keep.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10809f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238233c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1a410",
   "metadata": {},
   "source": [
    "### Text encoding\n",
    "\n",
    "Our classifier will be built (trained) using the tokenized and normalized abstracts. However, we need first to convert them into numbers so a classifier con work with them. This operation is called **encoding**.\n",
    "\n",
    "There are several ways of encoding texts. We'll use term-frequency inverse-document frequency (TF-IDF). TF-IDF transforms a text of words into a numeric vector where each word has a score. It gives a high score to words that show up a lot in a given document, but rarely across documents in the corpus (so they are more distinctive for the document only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66c8f1",
   "metadata": {},
   "source": [
    "We'll start by loading the library we'll use for the encoding and text classification: scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below for the installation:\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231813a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the encoder\n",
    "corpus = list(df2['abstract_tokenized'].apply(lambda x: ' '.join(x)))\n",
    "encoder = TfidfVectorizer(stop_words = ['paper'], max_features=1000) # initializing the encoder\n",
    "vectors = encoder.fit_transform(corpus)                              # encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c418495",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61306214",
   "metadata": {},
   "source": [
    "The resulting object `vectors` holds the encodings of the 95 abstracts. Each of them is a vector with the TF-IDF encoding of the 1000 more used words across the corpus. Choosing 1000 is arbitrary, remember we initially had a total of 4,500+ words --this was printed when we counted words and generated a dictionary with all the word counts. From now on, we'll refer to these 1000 words as our **dictionary**.\n",
    "\n",
    "For an easier understanding of the text encoding, we'll transform this back into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e88c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for the df\n",
    "words_encoded = encoder.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eedd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual content of the df\n",
    "vectors_data = vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(data=vectors_data, columns=words_encoded)\n",
    "df_tfidf.insert(0, 'id', df2['id']) # inserting the paper ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0f314",
   "metadata": {},
   "source": [
    "Two important points on this result:\n",
    "- The matrix `tf_idf` contains the same information as `vectors`, except that it's transformed into a Pandas dataframe with column names and the paper IDs. This step was not really necessary. We added it in order to better understand the encoding result. Most NLP programmers will omit this step and will work with `vectors` directly.\n",
    "- The resul in t`df_tfidf` and `vectors` is a matrix with **a lot** of zeroes. This happens because the encoding assigns a score of zero to the words that are part of the dictionary but not present in that paper. This is a usual result in TF-IDF encoding.\n",
    "\n",
    "The actual information in this data is sparse and spreads across the many dimensions (colums) of the data. We'll reduce the data dimensionality with principal component analysis (PCA) to only two dimensions. This will also allow us to visualize the proximity of each document.\n",
    "\n",
    "### Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f962a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "PCA = sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_tfidf.drop(['id'], axis=1)  # dropping the column id\n",
    "pca = PCA(n_components = 2).fit(data) # initializing and fitting the PCA transformer\n",
    "reduced_data = pca.transform(data)    # transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c26ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677c3ac",
   "metadata": {},
   "source": [
    "Note that the result of reducing the data dimensions is a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ed9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074e24f3",
   "metadata": {},
   "source": [
    "We can visualize the two resulting dimensions. The specific values of dimensions 1 and 2 are actually meaningless, but the **proximity of the observations** means that those abstracts had a close TF-IDF encoding, meaning that they're close in the words they contain.\n",
    "\n",
    "Also, remember that we obtained the first topic of each paper? we're going to use them as an approximation the true classes for our classification exercises. This will be noted in the color we use for the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing figure\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "plot = sns.scatterplot(x = reduced_data[:, 0], y = reduced_data[:, 1], hue = df2['first_topic'])\n",
    "\n",
    "# Aesthetics\n",
    "plt.legend(title='Topic')\n",
    "sns.move_legend(plot, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('True Classes')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b24ec",
   "metadata": {},
   "source": [
    "### Training a classifier for unsupervised classification\n",
    "\n",
    "Let's pause for a minute to go over the steps we've followed for text classification until this point:\n",
    "\n",
    "1. We started with the raw abstracts and did text data preparation:\n",
    "    - converted texts to lowercase\n",
    "    - removed stop words and numbers\n",
    "    - lemmatized words\n",
    "2. Then we encoded the prepared data using TF-IDF\n",
    "3. Next, we reduced the dimensions from 1000 to 2 and visualized the result\n",
    "\n",
    "We're going to continue using the results from (3) to train our unsupervised and supervised classifiers. We'll start with the unsupervised approach, building clusters of the abstracts that are close to each other in the PCA results. We're going to use the method `Kmeans()` from the module `cluster` of the library `sklearn` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf338e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df2['first_topic'].unique())                        # same n of clusters as topics: 8\n",
    "km = sklearn.cluster.KMeans(n_clusters=n, init='k-means++') # initializing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d467e0",
   "metadata": {},
   "source": [
    "### Unsupervised classification\n",
    "\n",
    "Once the classifier is fitted with new data, the cluster classification will be stored in the atrribute `labels_` of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177db24",
   "metadata": {},
   "source": [
    "We can use this to plot the predicted classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d388f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing figure\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "plot = sns.scatterplot(x = reduced_data[:, 0], y = reduced_data[:, 1], hue = km.labels_, palette=\"deep\")\n",
    "\n",
    "# Aesthetics\n",
    "plt.legend(title='Class')\n",
    "sns.move_legend(plot, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Predicted Classes - Clustering')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3348e7",
   "metadata": {},
   "source": [
    "Note that the predicted classes don't have a label that clearly corresponds to the actual classes --the topics. This is because clustering is an unsupervised method of classification: it only builds groups based on proximity but doesn't label what each group is.\n",
    "\n",
    "### Training a classifier for supervised classification\n",
    "\n",
    "To assign observations to labeled groups, we need to do supervised classification. We'll use a random forest classifier in this example, but other types of classifiers are available in the library we're using (scikit learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff193f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdf8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = reduced_data          # the PCA result\n",
    "y = df2['first_topic']    # the predicted labels\n",
    "classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762fdd8",
   "metadata": {},
   "source": [
    "After this, `classifier` has been trained with the data in `x` to know which patterns in it produce the results in `y`.\n",
    "\n",
    "### Supervised classification\n",
    "\n",
    "Now we'll classify our texts with the classifier we trained. Given that it was trained with encoded normalized words, the input for any classification should also be encoded normalized words. We'll use our same data of `tf_idf` to produce a classification and will compare it the actual true values to have a sense of how well this classifier performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df2[['title', 'first_topic']]\n",
    "df_predictions['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions['correct'] = False\n",
    "df_predictions.loc[df_predictions['first_topic'] == df_predictions['predictions'], 'correct'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a216d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932848a",
   "metadata": {},
   "source": [
    "Some notes on this result:\n",
    "\n",
    "- Our classifier is only 43% accurate. This is not a good performance but we had to work with very small data that we can manage in a short training session. In a real setting, you should have ideally with 1,000+ observations and different types of classifiers.\n",
    "- We are using our classifier on the same data we used for training it. In a real setting, this is a very bad practice as it will likely lead to overfitting: producing a classifier that works perfectly well for the data it was trained on but can't generalize for out-of-sample cases. The way you avoid this is by separating your data in a training dataset and a test dataset. Then you use the training set for training and the test set for evaluating its performance.\n",
    "- You can add to the the PCA vectors or TF-IDF matrix other data that will probably have predicting power for the variable we classify. Remember we extracted the JEL topics before? those are probably good predictors in this case.\n",
    "\n",
    "Visualizing the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "plot = sns.scatterplot(x = reduced_data[:, 0], y = reduced_data[:, 1], hue = df_predictions['predictions'], palette=\"deep\")\n",
    "\n",
    "# Aesthetics\n",
    "plt.legend(title='Class')\n",
    "sns.move_legend(plot, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Predicted Classes - Naive Bayes')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8218cd",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "\n",
    "### Other languages\n",
    "\n",
    "These exercises used a corpus in English. However, the principles for working with other languages are just the same for all of these text classification tasks. SpaCy has NLP models in other languages available, you can check them [here](https://spacy.io/usage/models).\n",
    "\n",
    "### Other text analysis tasks\n",
    "\n",
    "This was an overview of possibly the simplest text analysis tasks. Other tasks are:\n",
    "\n",
    "- Named entity recognition: detecting mentions of a meaningful entity (places, names of people, dates, etc) in texts\n",
    "- Vector spaces and word embeddings: transforming texts or words into vectors of \"meanings\". You can then work with them for other tasks, such as compare the proximity of texts based on meanings\n",
    "- Generative AI with texts: generating texts based on prompts or previous text.\n",
    "\n",
    "### Large Language Models (LLMs)\n",
    "\n",
    "We didn't cover LLMs because they're not part of an introductory session. If you're more interested in learning about them, we recommend these readings:\n",
    "\n",
    "- BERT was the first (or at least one of the first?) LLM publicly released. This article explains well how it works: [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
    "- This is a tutorial of how to work with BERT to fine-tune it for specific NLP/text analysis tasks: [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ce-text-analysis]",
   "language": "python",
   "name": "conda-env-ce-text-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
